{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9636307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cpu False\n",
      "gcc (crosstool-NG 1.23.0.449-a04d0) 7.3.0\n",
      "Copyright (C) 2017 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59271904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb00e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = get_cfg()\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.MODEL.DEVICE='cpu'\n",
    "predictor = DefaultPredictor(cfg)\n",
    "# outputs = predictor(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a383c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "# register_coco_instances(\"my_dataset_train\", {}, \"json_annotation_train.json\", \"path/to/image/dir\")\n",
    "register_coco_instances(\"test_keypoint\", {}, \"/HOMES/sichengliu/Train_Image/test_keypoint.json\", \"/HOMES/sichengliu/Train_Image/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6032ae45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[05/25 11:26:21 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[05/25 11:26:21 d2.data.datasets.coco]: \u001b[0mLoaded 48 images in COCO format from /HOMES/sichengliu/Train_Image/test_keypoint.json\n",
      "\u001b[32m[05/25 11:26:21 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   person   | 207          |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[05/25 11:26:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[05/25 11:26:21 d2.data.common]: \u001b[0mSerializing 48 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[05/25 11:26:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.11 MiB\n",
      "\u001b[32m[05/25 11:26:21 d2.evaluation.evaluator]: \u001b[0mStart inference on 48 images\n",
      "\u001b[32m[05/25 11:26:58 d2.evaluation.evaluator]: \u001b[0mInference done 11/48. 3.0824 s / img. ETA=0:01:54\n",
      "\u001b[32m[05/25 11:27:04 d2.evaluation.evaluator]: \u001b[0mInference done 13/48. 3.0562 s / img. ETA=0:01:47\n",
      "\u001b[32m[05/25 11:27:10 d2.evaluation.evaluator]: \u001b[0mInference done 15/48. 3.1033 s / img. ETA=0:01:42\n",
      "\u001b[32m[05/25 11:27:17 d2.evaluation.evaluator]: \u001b[0mInference done 17/48. 3.1579 s / img. ETA=0:01:37\n",
      "\u001b[32m[05/25 11:27:25 d2.evaluation.evaluator]: \u001b[0mInference done 19/48. 3.2750 s / img. ETA=0:01:35\n",
      "\u001b[32m[05/25 11:27:33 d2.evaluation.evaluator]: \u001b[0mInference done 21/48. 3.3331 s / img. ETA=0:01:30\n",
      "\u001b[32m[05/25 11:27:40 d2.evaluation.evaluator]: \u001b[0mInference done 23/48. 3.3628 s / img. ETA=0:01:24\n",
      "\u001b[32m[05/25 11:27:49 d2.evaluation.evaluator]: \u001b[0mInference done 25/48. 3.4726 s / img. ETA=0:01:19\n",
      "\u001b[32m[05/25 11:27:56 d2.evaluation.evaluator]: \u001b[0mInference done 27/48. 3.4789 s / img. ETA=0:01:13\n",
      "\u001b[32m[05/25 11:28:04 d2.evaluation.evaluator]: \u001b[0mInference done 29/48. 3.5128 s / img. ETA=0:01:06\n",
      "\u001b[32m[05/25 11:28:10 d2.evaluation.evaluator]: \u001b[0mInference done 31/48. 3.4867 s / img. ETA=0:00:59\n",
      "\u001b[32m[05/25 11:28:17 d2.evaluation.evaluator]: \u001b[0mInference done 33/48. 3.4906 s / img. ETA=0:00:52\n",
      "\u001b[32m[05/25 11:28:24 d2.evaluation.evaluator]: \u001b[0mInference done 35/48. 3.4755 s / img. ETA=0:00:45\n",
      "\u001b[32m[05/25 11:28:31 d2.evaluation.evaluator]: \u001b[0mInference done 37/48. 3.4718 s / img. ETA=0:00:38\n",
      "\u001b[32m[05/25 11:28:37 d2.evaluation.evaluator]: \u001b[0mInference done 39/48. 3.4597 s / img. ETA=0:00:31\n",
      "\u001b[32m[05/25 11:28:43 d2.evaluation.evaluator]: \u001b[0mInference done 41/48. 3.4445 s / img. ETA=0:00:24\n",
      "\u001b[32m[05/25 11:28:50 d2.evaluation.evaluator]: \u001b[0mInference done 43/48. 3.4251 s / img. ETA=0:00:17\n",
      "\u001b[32m[05/25 11:28:56 d2.evaluation.evaluator]: \u001b[0mInference done 45/48. 3.4210 s / img. ETA=0:00:10\n",
      "\u001b[32m[05/25 11:29:03 d2.evaluation.evaluator]: \u001b[0mInference done 47/48. 3.4269 s / img. ETA=0:00:03\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:26.948413 (3.417405 s / img per device, on 1 devices)\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:02:26 (3.415341 s / img per device, on 1 devices)\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.01 seconds.\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.818\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.492\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.257\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.517\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.501\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.567\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.360\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.602\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 48.136 | 81.777 | 49.229 |  nan  | 25.720 | 51.749 |\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *keypoints*\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.655\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.795\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.671\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.256\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.724\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.723\n",
      " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.841\n",
      " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.739\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.367\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.783\n",
      "\u001b[32m[05/25 11:29:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for keypoints: \n",
      "|   AP   |  AP50  |  AP75  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|\n",
      "| 65.527 | 79.461 | 67.113 | 25.570 | 72.415 |\n",
      "OrderedDict([('bbox', {'AP': 48.135939346929604, 'AP50': 81.77664829896455, 'AP75': 49.22892139618969, 'APs': nan, 'APm': 25.720390526554176, 'APl': 51.74915722499635}), ('keypoints', {'AP': 65.5273975321395, 'AP50': 79.46095740908856, 'AP75': 67.11330114113822, 'APm': 25.5703577380397, 'APl': 72.41520683417862})])\n"
     ]
    }
   ],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"test_keypoint\", (\"bbox\", \"keypoints\"), False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"test_keypoint\")\n",
    "# print(val_loader)\n",
    "# print(evaluator)\n",
    "print(inference_on_dataset(predictor.model, val_loader, evaluator))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
